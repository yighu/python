{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Logistic_Regression.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4dO65skF3rAci11oNGj0X"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Qgb_jDTkumCh"},"source":["#What is logistic regression?\n","#TYPES OF LOGISTIC REGRESSION\n","##Binary Logistic Regression, in which the target variable has only two possible values, e.g., pass/fail or win/lose.\n","##Multi Logistic Regression, in which the target variable has three or more possible values that are not ordered, e.g., sweet/sour/bitter or cat/dog/fox.\n","##Ordinal Logistic Regression, in which the outputs are ordered in some way, e.g., bad/good/better/best or low/medium/high.\n","\n","geeksforgeeks.org/ml-logistic-regression-using-tensorflow/\n"]},{"cell_type":"markdown","metadata":{"id":"CkXZmYKwqPyj"},"source":["Logistic Regression \n"]},{"cell_type":"code","metadata":{"id":"LBPw571rqTnY"},"source":["import numpy as np \n","import matplotlib.pyplot as plt \n","\n","def sigmoid(z): \n","\treturn 1 / (1 + np.exp( - z)) \n","\n","plt.plot(np.arange(-5, 5, 0.1), sigmoid(np.arange(-5, 5, 0.1))) \n","plt.title('Visualization of the Sigmoid Function') \n","\n","plt.show() \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tPaikDK-ruYl"},"source":["geeksforgeeks.org/ml-logistic-regression-using-tensorflow/\n"]},{"cell_type":"markdown","metadata":{"id":"PVMp9dbNvtwk"},"source":["#BUILDING LOGISTIC REGRESSION USING TENSORFLOW 2.0.\n","##STEP 1: IMPORTING NECESSARY MODULES"]},{"cell_type":"code","metadata":{"id":"V4ZVFRpCryXT"},"source":["from __future__ import absolute_import, division, print_function\n","\n","import tensorflow as tf\n","\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3bNHdlCnwNrG"},"source":["##STEP 3: SETTING UP HYPERPARAMETERS AND DATA SET PARAMETERS"]},{"cell_type":"code","metadata":{"id":"AIW_cYxqsbUF"},"source":["# MNIST dataset parameters.\n","\n","num_classes = 10 # 0 to 9 digits\n","\n","num_features = 784 # 28*28\n","\n","# Training parameters.\n","\n","learning_rate = 0.01\n","\n","training_steps = 1000\n","\n","batch_size = 256\n","\n","display_step = 50"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zly3UmX8v91N"},"source":["##STEP 2: LOADING AND PREPARING THE MNIST DATA SET\n"]},{"cell_type":"code","metadata":{"id":"Twrcfvgnr474"},"source":["from tensorflow.keras.datasets import mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Convert to float32.\n","\n","x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n","\n","# Flatten images to 1-D vector of 784 features (28*28).\n","\n","x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n","\n","# Normalize images value from [0, 255] to [0, 1].\n","\n","x_train, x_test = x_train / 255., x_test / 255."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vBFT-5E-wWrn"},"source":["##STEP 4: SHUFFLING AND BATCHING THE DATA\n"]},{"cell_type":"code","metadata":{"id":"OFxHPqnnshun"},"source":["# Use tf.data API to shuffle and batch data.\n","\n","train_data=tf.data.Dataset.from_tensor_slices((x_train,y_train))\n","\n","train_data=train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FgwD1Foewag0"},"source":["##STEP 5: INITIALIZING WEIGHTS AND BIASES\n"]},{"cell_type":"code","metadata":{"id":"0TroIEdksnBX"},"source":["# Weight of shape [784, 10], the 28*28 image features, and a total number of classes.\n","\n","W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n","\n","# Bias of shape [10], the total number of classes.\n","\n","b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f7NTXcguwfA8"},"source":["##STEP 6: DEFINING LOGISTIC REGRESSION AND COST FUNCTION\n"]},{"cell_type":"code","metadata":{"id":"OvJVjofgsvOw"},"source":["# Logistic regression (Wx + b).\n","\n","def logistic_regression(x):\n","\n","    # Apply softmax to normalize the logits to a probability distribution.\n","\n","    return tf.nn.softmax(tf.matmul(x, W) + b)\n","\n","# Cross-Entropy loss function.\n","\n","def cross_entropy(y_pred, y_true):\n","\n","    # Encode label to a one hot vector.\n","\n","    y_true = tf.one_hot(y_true, depth=num_classes)\n","\n","    # Clip prediction values to avoid log(0) error.\n","\n","    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n","\n","    # Compute cross-entropy.\n","\n","    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_DaaUxywj1f"},"source":["STEP 7: DEFINING OPTIMIZERS AND ACCURACY METRICS\n"]},{"cell_type":"code","metadata":{"id":"a4AkvHmHs20L"},"source":["# Accuracy metric.\n","\n","def accuracy(y_pred, y_true):\n","\n","# Predicted class is the index of the highest score in prediction vector (i.e. argmax).\n","\n","  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n","\n","  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","# Stochastic gradient descent optimizer.\n","\n","optimizer = tf.optimizers.SGD(learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4iqD6EVcwn1z"},"source":["##STEP 8: OPTIMIZATION PROCESS AND UPDATING WEIGHTS AND BIASES"]},{"cell_type":"code","metadata":{"id":"5nUZZQW_tCbR"},"source":["# Optimization process. \n","\n","def run_optimization(x, y):\n","\n","# Wrap computation inside a GradientTape for automatic differentiation.\n","\n","    with tf.GradientTape() as g:\n","\n","        pred = logistic_regression(x)\n","\n","        loss = cross_entropy(pred, y)\n","\n","    # Compute gradients.\n","\n","    gradients = g.gradient(loss, [W, b])\n","\n","  \n","\n","    # Update W and b following gradients.\n","\n","    optimizer.apply_gradients(zip(gradients, [W, b]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VkRPunNKwroy"},"source":["##STEP 9: THE TRAINING LOOP"]},{"cell_type":"code","metadata":{"id":"1LbKgKeVtHGG"},"source":["# Run training for the given number of steps.\n","\n","for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n","\n","    # Run the optimization to update W and b values.\n","\n","    run_optimization(batch_x, batch_y)\n","\n","    \n","\n","    if step % display_step == 0:\n","\n","        pred = logistic_regression(batch_x)\n","\n","        loss = cross_entropy(pred, batch_y)\n","\n","        acc = accuracy(pred, batch_y)\n","\n","        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lgy2q4vtwwzV"},"source":["##STEP 10: TESTING MODEL ACCURACY USING THE TEST DATA\n"]},{"cell_type":"code","metadata":{"id":"6-7g79Y1tRlv"},"source":["# Test model on validation set.\n","\n","pred = logistic_regression(x_test)\n","\n","print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgviEV0muk1z"},"source":[""]}]}